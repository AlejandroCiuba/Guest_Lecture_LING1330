{"cells":[{"cell_type":"markdown","metadata":{"id":"N6BGVQF3z0Mb"},"source":["# Machine Learning & Neural Networks Demonstration\n","***\n","By [Alejandro Ciuba](https://alejandrociuba.github.io), alejandrociuba@gmail.com\n","***\n","## Table of Contents\n","1. [Scikit Learn's Naive Bayes' Classifier - Austen vs. Melville](#1-scikit-learns-naive-bayes-classifier---austen-vs-melville)\n","2. [PyTorch (Small FFNN Model)](#2-pytorch-small-ffnn-model)\n","3. [Hugging Face Demo](#3-hugging-face-demo)\n","***\n","## Necessary Imports & Installs\n","These imports will be used throughout the whole notebook. This code cell should be ran after every restart of the kernel"]},{"cell_type":"code","source":["# Install Hugging Face modules\n","! pip install transformers\n","! pip install evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u5CcAtFO5hc8","executionInfo":{"status":"ok","timestamp":1669870059613,"user_tz":300,"elapsed":9420,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"43d66f5c-66b2-40b4-887e-7ffd3cf327dc"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.24.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.25.11)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.8/dist-packages (0.3.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from evaluate) (4.64.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.70.14)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.7.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.21.6)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.23.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from evaluate) (3.1.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.11.1)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2022.11.0)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.18.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.3.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from evaluate) (21.3)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.1.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->evaluate) (3.0.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (1.25.11)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2022.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\n"]}]},{"cell_type":"code","execution_count":56,"metadata":{"id":"Kc2GwTtsz0Me","executionInfo":{"status":"ok","timestamp":1669870059614,"user_tz":300,"elapsed":7,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}}},"outputs":[],"source":["# General imports\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","import random as rand\n","import nltk\n","\n","# Scikit Learn\n","from sklearn.model_selection import train_test_split # We'll use this a lot even though it's an sklearn function\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import make_pipeline\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","\n","# PyTorch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch\n","\n","# Hugging Face\n","from transformers import pipeline\n","import evaluate"]},{"cell_type":"code","source":["# Download the gutenberg corpus and punkt tokenizer\n","nltk.download('gutenberg')\n","nltk.download('punkt')\n","from nltk.corpus import gutenberg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l0-GLQBw1PVY","executionInfo":{"status":"ok","timestamp":1669868330930,"user_tz":300,"elapsed":223,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"650a9a4d-7e31-407a-cdb8-9e9d1bac3cea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Package gutenberg is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","metadata":{"id":"b38GUfBYz0Mf"},"source":["***\n","## 1. Scikit Learn's Naive Bayes' Classifier - Austen vs. Melville\n","Here, we'll revisit the task of labelling a given sentence as either a sentence written by Jane Austen or Herman Melville. However, this time we'll use _Scikit Learn's_ [_Categorical Naive Bayes' Classifier_](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html#sklearn.naive_bayes.CategoricalNB), a NB Classifier tailored to categorical data-labelling tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PkEWKWyKz0Mg","executionInfo":{"status":"ok","timestamp":1669867191262,"user_tz":300,"elapsed":1589,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"c3a55f5d-d464-40e6-af84-50f979667ab4"},"outputs":[{"output_type":"stream","name":"stdout","text":["17776 17776\n","(17776,)\n","But now comes the greatest joke of the dream , Flask . melville\n","<class 'numpy.ndarray'>\n"]}],"source":["# First, we need to get the data, the individual sentences\n","a_sents = [' '.join(sent) for sent in gutenberg.sents('austen-emma.txt')]\n","m_sents = [' '.join(sent) for sent in gutenberg.sents('melville-moby_dick.txt')]\n","\n","# Turn it into an np.array\n","all_sents = np.concatenate([a_sents, m_sents])\n","\n","# Make an np.array for the labels\n","all_labels = np.array(['austen'] * len(a_sents) + ['melville'] * len(m_sents))\n","\n","print(len(all_sents), len(all_labels))\n","print(all_sents.shape)\n","print(all_sents[10192], all_labels[10192])\n","print(type(all_sents))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kjGST69Mz0Mh","executionInfo":{"status":"ok","timestamp":1669867196693,"user_tz":300,"elapsed":340,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"44761ba9-6c60-4d9d-ade1-b105baba587e"},"outputs":[{"output_type":"stream","name":"stdout","text":["11850 5926\n","But now comes the greatest joke of the dream , Flask . melville\n"]}],"source":["# Now, we need to do our train-test split. Conveniently, sklearn has a function just for that\n","X_train, X_test, y_train, y_test = train_test_split(all_sents, all_labels, test_size=1/3, random_state=0)\n","\n","print(len(X_train), len(X_test))\n","print(X_train[314], y_train[314])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fVv5itriz0Mi","executionInfo":{"status":"ok","timestamp":1669867200784,"user_tz":300,"elapsed":498,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"61d82d21-24c1-4f0e-81fe-0c6ae69a88df"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(steps=[('tfidfvectorizer',\n","                 TfidfVectorizer(max_features=1500, stop_words='english')),\n","                ('multinomialnb', MultinomialNB())])"]},"metadata":{},"execution_count":19}],"source":["# We need to create a pipeline! This pipeline will automatically tokenize our data and then feed it into the model\n","nbmodel = make_pipeline(TfidfVectorizer(max_features=1500, stop_words='english'), MultinomialNB())\n","\n","# Now, we need to train the model by fitting the training data to it. Very simple in sklearn!\n","nbmodel.fit(X_train, y_train)\n","\n","nbmodel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":345},"id":"gPCgYVchz0Mj","executionInfo":{"status":"ok","timestamp":1669867204733,"user_tz":300,"elapsed":639,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"79e15ad9-d66a-4ff1-88a9-5357086ee0df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 90.432% \n"]},{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'Heatmap of predicted vs. correct authors\\non Austen vs. Melville')"]},"metadata":{},"execution_count":20},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# Now let's test\n","plabels = nbmodel.predict(X_test)\n","\n","# Get base accuracy\n","acc = accuracy_score(y_test, plabels) * 100\n","print(f\"Accuracy: {acc:.{5}}% \")\n","\n","# Create a heatmap to examine which are right and wrong\n","mat = confusion_matrix(y_test, plabels)\n","sns.heatmap(mat, square=True, annot=True, fmt='d', cmap=\"Reds\")\n","plt.xlabel('correct label')\n","plt.ylabel('predicted label')\n","plt.title('Heatmap of predicted vs. correct authors\\non Austen vs. Melville')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XDdG824tz0Mj","executionInfo":{"status":"ok","timestamp":1669867208506,"user_tz":300,"elapsed":528,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"19589d6d-fa66-4013-c5a2-cac9c6807c25"},"outputs":[{"output_type":"stream","name":"stdout","text":["['melville']\n","['austen']\n","['melville']\n"]}],"source":["# Let's try it ourselves\n","print(nbmodel.predict([\"Wow, this was a really cool thing to do and wasn't hard to set up!\"]))\n","print(nbmodel.predict([\"I am absolutely crushed Mr. Bayes rejected my invitation to the destruction of the world!\"]))\n","print(nbmodel.predict([\"This is Mr. Herman Austen writing. How goes your day, my friend, Queequeg?\"])) # Notice how one feature holds a lot of power?"]},{"cell_type":"markdown","metadata":{"id":"eIqiyAZ9z0Mk"},"source":["***\n","## 2. PyTorch (Small FFNN Model)\n","This will be a simple _Feed Forward Neural Network_ with fully connected layers. This will demonstrate how neural networks are bad at deterministic tasks like logic operators. Here, we will try to train a neural network to recognize the correct output of a logical operator given two binary numbers (AND, OR, XOR). To create our data, the format will be a vector of length 3 such that:\n","\n","`[int([0,1]), int([0,1]), int([0,2])]` where the first two numbers are either 0 or 1 and the third corresponds to the logical operator (AND = 0, OR = 1, XOR = 2)\n","\n","### Doing Things The Old-Fashioned Way"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y_cnju6Gz0Mk","executionInfo":{"status":"ok","timestamp":1669867212420,"user_tz":300,"elapsed":224,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"18f7ac33-9a38-4d10-f7fe-c1b53fffa4f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["(10000, 3)\n","[0 1 1]\n","(10000,)\n","[0 1 1] 1\n"]}],"source":["# Function to create output labels (look we did it, lmfao)\n","def output_old(nums:list, op:int) -> int:\n","    if op == 0:\n","        return nums[0] & nums[1]\n","    elif op == 1:\n","        return nums[0] | nums[1]\n","    else:\n","        return nums[0] ^ nums[1]\n","\n","# Let's create our training and testing data\n","size = 10000\n","num1 = [rand.randint(0, 1) for i in range(size)]\n","num2 = [rand.randint(0, 1) for i in range(size)]\n","op = [rand.randint(0, 2) for i in range(size)]\n","dataset = np.array([[num1, num2, op] for num1, num2, op in zip(num1, num2, op)])\n","print(dataset.shape)\n","print(dataset[100])\n","\n","# Now for the labels\n","labels = np.array([output_old([data[0], data[1]], data[2]) for data in dataset])\n","print(labels.shape)\n","print(dataset[100], labels[100])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKxlCb5lz0Ml","executionInfo":{"status":"ok","timestamp":1669867215433,"user_tz":300,"elapsed":2,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"2a04989d-acbd-4241-d460-21487b226d64"},"outputs":[{"output_type":"stream","name":"stdout","text":["7500 2500\n","[0 1 0] 0\n"]}],"source":["# Let's split it into our train-test split\n","X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=1/4, random_state=0)\n","\n","print(len(X_train), len(X_test))\n","print(X_train[314], y_train[314])"]},{"cell_type":"markdown","metadata":{"id":"YkObUCjNz0Ml"},"source":["### Now For The PyTorch Way"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XnwujCcwz0Mm","executionInfo":{"status":"ok","timestamp":1669870070817,"user_tz":300,"elapsed":1086,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"5be1c689-c3ba-4c90-d5af-739152309871"},"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor([1, 1, 1]), tensor(1))\n"]}],"source":["# Create a specialized dataset class to make the training loop easier\n","# Class also modifed to have transforms\n","class BinaryOpsDataset(Dataset):\n","    def __init__(self, n=100, transform=None):\n","\n","        # Let's create our training and testing data\n","        self.n = n\n","        num1 = [rand.randint(0, 1) for i in range(n)]\n","        num2 = [rand.randint(0, 1) for i in range(n)]\n","        op = [rand.randint(0, 2) for i in range(n)]\n","        self.x = np.array([[num1, num2, op] for num1, num2, op in zip(num1, num2, op)])\n","\n","        # Now for the labels\n","        self.y = np.array([self.output([data[0], data[1]], data[2]) for data in self.x])\n","\n","        # And any necessary transformations\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return self.n\n","\n","    def __getitem__(self, index):\n","\n","        if self.transform:\n","            return self.transform((self.x[index], self.y[index]))\n","        else:\n","            return self.x[index], self.y[index]\n","\n","    def output(self, nums:list, op:int) -> int:\n","        \"\"\"\n","        Same function as before, just made a class method.\n","        \"\"\"\n","\n","        if op == 0:\n","            return int(nums[0]) & int(nums[1])\n","        elif op == 1:\n","            return int(nums[0]) | int(nums[1])\n","        else:\n","            return int(nums[0]) ^ int(nums[1])\n","\n","# Create a custom transforms\n","class ToTensor():\n","    def __call__(self, sample):\n","        inputs, targets = sample\n","        return torch.from_numpy(np.asarray(inputs)), torch.from_numpy(np.asarray(targets))\n","\n","# Create our dataset, contains data\n","n = 50000\n","train_dataset = BinaryOpsDataset(n, transform=ToTensor())\n","test_dataset = BinaryOpsDataset(n, transform=ToTensor())\n","print(train_dataset[100])\n","\n","\n","# Create our dataloader, used to iterate through the training loop more easily\n","train_dataloader = DataLoader(dataset=train_dataset, batch_size=40, shuffle=True, num_workers=2) # Subprocs\n","test_dataloader = DataLoader(dataset=test_dataset, batch_size=40, shuffle=True, num_workers=2) # Subprocs"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"tkiNfU7rz0Mm","executionInfo":{"status":"ok","timestamp":1669870119870,"user_tz":300,"elapsed":214,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}}},"outputs":[],"source":["# Now this is where things get a little bit complicated. We need to make a class to store the neural network\n","# First, here are all the hyperparameters it'll have\n","# Length of a datapoint\n","input_size = 3\n","\n","# How many \"nodes\" each hidden layer will have, kind of arbitrary without optimization searches\n","hidden_size = 16\n","\n","# The number of classes it'll be asked to identify (2)\n","num_classes = 2\n","\n","# Learning rate, how quickly it should \"accept/learn\" new data (the step size of our gradient descent)\n","learning_rate = .001\n","\n","# How many epochs (iteration through the entire training data) there are\n","num_epochs = 6\n","\n","# Create the Neural Network, a fully connected FFNN (Perceptron)\n","class FFNN(nn.Module):\n","\n","    def __init__(self, input_size, hidden_size, num_classes, /):\n","\n","        # This is required for any NN class\n","        super(FFNN, self).__init__()\n","\n","        # Create layers\n","        self.l1 = nn.Linear(input_size, hidden_size)\n","\n","        # The activation function used between each layer\n","        self.activation = nn.ReLU()\n","\n","        self.l2 = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        This is the function called for a prediction, feeding the data through our model in-order\n","        \"\"\"\n","        \n","        out = self.l1(x)\n","        out = self.activation(out)\n","        out = self.l2(out)\n","        return out\n","\n","# Create our model\n","model = FFNN(input_size, hidden_size, num_classes)\n","\n","# Loss and Optimizer\n","# Loss is how we calculate the loss (the loss function) and optimizer is an additional step for performance\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKGhitMsz0Mn","executionInfo":{"status":"ok","timestamp":1669870144693,"user_tz":300,"elapsed":23027,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"4752cb3b-2890-400d-bd23-a47d000a3913"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1/6, step: 625/1250, loss=0.4200f\n","epoch: 1/6, step: 1250/1250, loss=0.1397f\n","epoch: 2/6, step: 625/1250, loss=0.0710f\n","epoch: 2/6, step: 1250/1250, loss=0.0320f\n","epoch: 3/6, step: 625/1250, loss=0.0158f\n","epoch: 3/6, step: 1250/1250, loss=0.0091f\n","epoch: 4/6, step: 625/1250, loss=0.0065f\n","epoch: 4/6, step: 1250/1250, loss=0.0038f\n","epoch: 5/6, step: 625/1250, loss=0.0022f\n","epoch: 5/6, step: 1250/1250, loss=0.0015f\n","epoch: 6/6, step: 625/1250, loss=0.0012f\n","epoch: 6/6, step: 1250/1250, loss=0.0008f\n"]}],"source":["# Harness GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Move model to device\n","model = model.to(device)\n","\n","# Training loop\n","total_samples = len(train_dataset)\n","\n","# Steps per epoch\n","n_total_steps = len(train_dataloader)\n","\n","for epoch in range(num_epochs):\n","    for i, (logic_ops, labels) in enumerate(train_dataloader):\n","\n","        # Convert to float32 and push to GPU\n","        inputs = logic_ops.to(torch.float32).to(device)\n","        labels = labels.to(device)\n","\n","        # Forward and Loss        \n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward Pass\n","        optimizer.zero_grad() # Clear grads from last cycle\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 625 == 0:\n","            print('epoch: %d/%d, step: %d/%d, loss=%.4ff' % (epoch+1, num_epochs, i+1, n_total_steps, loss.item()))"]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EdIpE3A8z0Mn","executionInfo":{"status":"ok","timestamp":1669870149550,"user_tz":300,"elapsed":3925,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"f3489a95-7eb0-4a2b-cf57-980a2c09c0c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy=100.0000\n"]}],"source":["# Testing\n","# No grad means it doesn't save data for back propagation\n","with torch.no_grad():\n","    n_correct = 0\n","    n_samples = 0\n","\n","    for logic_ops, labels in test_dataloader:\n","\n","        # Convert to float32 and push to GPU\n","        inputs = logic_ops.to(torch.float32).to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(inputs)\n","        \n","        # Since the cross-entropy loss also applied the softmax,\n","        # We need to interpret the results ourselves here.\n","        # Rows of vectors, we want the max from the vectors\n","        # max_value, index of value respective to its demension\n","        _, preds = torch.max(outputs, 1)\n","        n_samples += labels.shape[0]\n","        n_correct += (preds == labels).sum().item()\n","\n","    acc = 100.0 * n_correct / n_samples\n","    print('Accuracy=%.4f' % acc)"]},{"cell_type":"code","source":["    # Test our model ourselves\n","    input = [1,0,2]\n","    op = ''\n","    \n","    if input[2] == 0:\n","        op = '&'\n","    elif input[2] == 1:\n","        op = '|'\n","    elif input[2] == 2:\n","        op = '^'\n","    else:\n","        op = 'WHAT'\n","\n","    \"\"\"\n","    Looks complicated, but this is just a format string saying take our input, format it like integer operator integer.\n","    Then, take the model's output, find out the answer by getting the index of the max value (the 2nd item in the tuple)\n","    and then cast that to an int to make it consistent with the rest. Remember, linear layers like floats, so when we\n","    turn our input into a tensor for the model to run through itself, we need to cast the numbers as floats. We also\n","    need to push the tensor to the device we're using to make things consistent (and also utilize the GPU if we have one).\n","    \"\"\"\n","    print(\"%d %s %d = %d\" % (input[0], op, input[1], int(torch.max(model(torch.tensor(input, dtype=torch.float32).to(device)), 0)[1].item())))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yt5up7ld2H3d","executionInfo":{"status":"ok","timestamp":1669870216559,"user_tz":300,"elapsed":316,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"d442d305-5759-4473-8e1c-490300709e88"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["1 ^ 0 = 1\n"]}]},{"cell_type":"markdown","metadata":{"id":"JA3__oSnz0Mo"},"source":["***\n","## 3. Hugging Face Demo\n","This will be just a quick demo on how to use an _NLP-based Hugging Face Pretrained Transformer Model_. They are very easy to use and can be easily added to a given program to give it some more functionality.\n","\n","### Default Transformers"]},{"cell_type":"code","execution_count":68,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RzAmMfPDz0Mo","executionInfo":{"status":"ok","timestamp":1669870224375,"user_tz":300,"elapsed":2670,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"63ea4fc5-d2b5-4ece-accf-f0ca55653d8c"},"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]}],"source":["# First, just a general task but no Transformer specified; defaults to one\n","classifier = pipeline(\"sentiment-analysis\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qmwalF4dz0Mo","executionInfo":{"status":"ok","timestamp":1669867313779,"user_tz":300,"elapsed":268,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"cf64d405-15b7-4a55-afc2-56032f8f8484"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9995996356010437}]"]},"metadata":{},"execution_count":31}],"source":["classifier('This is my very first hugging face transformer model. It seems really nice to use!')"]},{"cell_type":"markdown","metadata":{"id":"VN5pXFGyz0Mp"},"source":["### Picking Our Own\n","The [website](https://huggingface.co) makes this very easy! Let's pick one! The one this notebook is currently using is the [_T5\\_Small_](https://huggingface.co/t5-small), but feel free to rewrite to pick your own!"]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lOwP9vF6z0Mp","executionInfo":{"status":"ok","timestamp":1669870235849,"user_tz":300,"elapsed":3176,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"739c9897-ac78-4f52-8a9f-d431ef08098e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/transformers/models/auto/modeling_auto.py:1132: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  warnings.warn(\n"]}],"source":["from transformers import AutoTokenizer, AutoModelWithLMHead\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n","\n","model = AutoModelWithLMHead.from_pretrained(\"t5-small\")"]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rLni5Zcz0Mp","executionInfo":{"status":"ok","timestamp":1669870237446,"user_tz":300,"elapsed":233,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"4f331556-740b-44e7-dcdf-bf1f837422b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Casa este minunată.\n"]}],"source":["# There's a lot of documentation on how to use this transformer (https://huggingface.co/docs/transformers/main/en/model_doc/t5#inference)\n","input_ids = tokenizer(\"translate English to Romanian: The house is wonderful.\", return_tensors=\"pt\").input_ids\n","outputs = model.generate(input_ids)\n","print(prediction := tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"markdown","source":["### Output Values\n","* `bleu (float)`: bleu score\n","* `precisions (list of floats)`: geometric mean of n-gram precisions,\n","* `brevity_penalty (float)`: brevity penalty,\n","* `length_ratio (float)`: ratio of lengths,\n","* `translation_length (int)`: translation_length,\n","* `reference_length (int)`: reference_length"],"metadata":{"id":"IXeA5vzf-T9i"}},{"cell_type":"code","source":["# Let's evaluate with BLEU (https://huggingface.co/spaces/evaluate-metric/bleu)\n","reference = [['Casa este mica']]\n","prediction = [prediction]\n","\n","# Load in BLEU\n","bleu = evaluate.load('bleu')\n","\n","print(bleu.compute(predictions=prediction, references=reference))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBGNOcwO6P8F","executionInfo":{"status":"ok","timestamp":1669870241705,"user_tz":300,"elapsed":1581,"user":{"displayName":"Alejandro Ciuba","userId":"08038255199237976900"}},"outputId":"ad973c99-55da-4e0a-8ce4-ce8fa60233a7"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["{'bleu': 0.0, 'precisions': [0.5, 0.3333333333333333, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.3333333333333333, 'translation_length': 4, 'reference_length': 3}\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"75564dbd713d8635e66bce18a5f6cdc021195db8d1ecf283647f3f759cb49a31"}},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}